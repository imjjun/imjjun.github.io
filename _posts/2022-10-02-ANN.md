---
title: Modern Computer Vision with Pytorch (1) Artificial Neural Network Fundamentals
tags: Modern_Computer_Vision
---

<strong>1. Artificial Neural Network Fundamentals</strong>

1-1) Forward 

layer가 한 층만 있는 ANN을 구현한다고 해보자. 이때, Forward 연산은 다음과 같이 수행된다.


```python
import numpy as np

def feed_forward(inputs, outputs, weights):
  pred_out=np.dot(inputs, weights[0])+weights[1]
  mean_squared_error=np.mean(np.square(pred_out-outputs))
  return mean_squared_error
```

input값 X와 W(가중치)와의 내적곱을 구한 후, weights[1]의 bias를 더한 값을 hidden layer 값으로 결정한다.
그 다음, 예측 값과 실제 output에 대하여 MSE를 구한 후, MSE값을 return한다.
이는 즉 Loss 함수로 연결되는 것이다.

1-2) Activation Function

Relu함수는 활성화함수로서 해당 노드를 활성화할지 말지, 결정하는 함수이다.

보통 음수값을 가지면 0으로 해당 노드값을 취하지 않고, 그 이상일 경우 양수인 다른 값을 가진다. 

가중치 곱으로 만들어지는 layer들의 output들에 대해서 Exploding하지 않도록 도와주고, 연산 속도 역시 빠르게 만들어준다는 특징이 있다(Negative value들에 대해서 0으로 바꿔주기 때문에)  layer 내에서 선형적 특징을 비선형적으로 바꿔주는 역할을 한다. 
특히 몇몇 ReLU 함수들의 경우, Derivative를 구하기가 쉬워 Computational benefit을 가질 수 있다.

1) tanh

탄젠트 하이퍼볼릭 함수로, Sigmoid Function과 유사한 형태를 가지고 있는 것이 특징이다.

![image](https://user-images.githubusercontent.com/90594374/193460028-c936cdf3-79ba-48b4-86f1-a09663d5b0c4.png)


2) Relu 함수

정류 선형 유닛( 영어: Rectified Linear Unit 렉티파이드 리니어 유닛 )에 대한 함수이다. 입력 값이 0보다 작으면 0으로 출력하고, 0보다 크면 해당 값을 출력하는, 말 그대로 ReLU의 기능성에 충실한 함수라고 할 수 있다. 해당 식은 다음과 같이 정의가능하기도 하다.

![image](https://user-images.githubusercontent.com/90594374/193460054-e7b8ce9a-ac29-49b1-92da-6f1ba4ed30a1.png)



3) Linear

입력받은 x를 그대로 전달하는 함수로, 선형적인 특성을 비선형적인 것으로 바꿔주는 ReLU함수에서, 선형성을 유지할 때 활용하는 함수이다. y=x, 일차함수의 형태와 동일하다.

4) Softmax

Softmax 역시 활성화 함수 중 하나이며, 마지막 layer에서 실제 output으로의 변환이 필요할 때 주로 활용되는 함수이다(표준화 role을 수행).

Softmax 함수를 거쳐 총합이 1로 표현되는 vector를 output으로 받기에, 그 중 가장 값이 큰 것(High Probability)에 대하여 해당 값이라고 Classification을 수행하기도 한다.

![image](https://user-images.githubusercontent.com/90594374/193460068-1630e5c9-0263-43aa-b4fe-c2f3feb1dbbc.png)



```python
def tanh(x):
  return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))

def relu(x):
  return np.where(x>0, x, 0)

def linear(x):
  return x

def softmax(x):
  return np.exp(x)/np.sum(np.exp(x))
```

1-3) Loss Function

훈련 마지막에 손실함수를 정의하면, 예측치와 정답치간의 오차율을 구할수있고, 이 오차율이 줄어드는 방향으로 BackPropagation 알고리즘을 수행할 수 있다.

딥러닝 훈련 특성상, Mini-Batch를 통해서 Matrix를 Batch 형태로 인풋으로 받는 경우가 대다수일 것이다. 이때 w의 shape에서 인풋의 개수 n 은 변하지 않는다. 그래서 인풋의 갯수가 n개라면 우리의 마지막 레이어를 통과한 예측값도 n x 1이 될것이고 그렇다면 각각의 데이터셋에 따른 오차율 또한 모두 다를 것이다. 그래서 각각의 오차율을 모두 더하고, 총 개수인 n으로 나누어주어 오차율의 평균을 구할 수 있다.

1) MSE

![image](https://user-images.githubusercontent.com/90594374/193460076-62f7f960-dfc2-4ba0-9dc3-e144a5ad0109.png)

2) MAE

![image](https://user-images.githubusercontent.com/90594374/193460082-936be3fb-86cc-4422-a50c-d780671d759e.png)

3) Binary Cross Entropy Loss Function

![image](https://user-images.githubusercontent.com/90594374/193460092-8e6ce10c-23b2-482c-a3d7-ea37eb0f290c.png)

4) Categorical Cross Entropy Loss Function

![image](https://user-images.githubusercontent.com/90594374/193460098-d6809920-e7c5-4573-8324-34c12850c5ee.png)


```python
def mse(p, y):
  return np.mean(np.square(p-y))

def mae(p,y):
  return np.mean(np.abs(p-y))

def binary_cross_entropy(p,y):
  return -np.mean(np.sum((y*np.log(p)+(1-y)*np.log(1-p))))

def categorical_cross_entropy(p,y):
  return -np.mean(np.sum(y*np.log(p)))
```

1-3) Update_weights(backward)

forward함수를 바탕으로 나온 loss에 기반하여, layer의 weight들을 update한다. 

x좌표 상에서 0.0001만큼의 차이를 바탕으로 Gradient(기울기)를 계산하고, learning rate를 곱하여 그만큼 weight를 update하는 식으로 backward 연산을 진행하여 loss를 줄여나간다.


```python
from copy import deepcopy
import numpy as np
def update_weights(inputs, outputs, weights, lr):
  original_weights=deepcopy(weights)
  original_loss=feed_forward(inputs, outputs, original_weights)
  updated_weights=deepcopy(weights)
  
  for i, layer in enumerate(original_weights):
    for index, weight in np.ndenumerate(layer):
      temp_weights=deepcopy(weights)
      temp_weights[i][index]+=0.0001
      _loss_plus=feed_forward(inputs, outputs, temp_weights)
      grad=(_loss_plus - original_loss)/(0.0001)
      updated_weights[i][index]-=grad*lr
  return updated_weights, original_loss
```

<strong>2) Practical Example


```python
x=[[1],[2],[3],[4]]
y=[[3],[6],[9],[12]]
W=[np.array([[0]], dtype=np.float32),np.array([[0]],dtype=np.float32)]
losses=[]
for epoch in range(100):
  W, loss=update_weights(x,y,W,0.01)
  losses.append([epoch,loss])
```


```python
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
loss_df=pd.DataFrame(losses,columns=['epoch','loss'])
sns.lineplot(x='epoch', y='loss',data=loss_df)
plt.title('Loss Value by Epoch')
plt.show()
```

![image](https://user-images.githubusercontent.com/90594374/193459981-30e59046-030e-422d-804d-198e9c320253.png)

